"""
Web vulnerability scanner implementation.
Scans web applications for security vulnerabilities.
"""

import asyncio
import re
from datetime import datetime
from typing import List, Set
from urllib.parse import urljoin, urlparse

import aiohttp
from bs4 import BeautifulSoup

from . import config
from .base import BaseScanner, VulnerabilityFinding


class WebScanner(BaseScanner):
    """Scanner for web application vulnerabilities."""

    def __init__(self):
        """Initialize web scanner."""
        super().__init__("web_vulnerability")

        # Track scanned URLs
        self.scanned_urls: Set[str] = set()
        self.found_urls: Set[str] = set()

        # Track scan statistics
        self.stats = {"requests_made": 0, "urls_found": 0, "vulnerabilities_found": 0}

        # Initialize session
        self.session = None
        self.rate_limiter = None

    def prepare_scan(self) -> None:
        """Prepare for web scanning."""
        self.logger.info("Preparing web vulnerability scan")

        # Create aiohttp session
        self.session = aiohttp.ClientSession(
            headers={"User-Agent": "DojoPool-Security-Scanner/1.0", "Accept": "*/*"}
        )

        # Initialize rate limiter
        self.rate_limiter = asyncio.Semaphore(self.config["max_requests_per_second"])

    async def run_scan(self) -> List[VulnerabilityFinding]:
        """Run web vulnerability scan."""
        findings = []

        try:
            base_url = config.SCAN_TARGETS["web_interface"]["base_url"]
            self.found_urls.add(base_url)

            # Crawl and scan site
            async for url in self._crawl_site(base_url):
                if url not in self.scanned_urls:
                    scan_findings = await self._scan_url(url)
                    findings.extend(scan_findings)
                    self.scanned_urls.add(url)

            self.logger.info(
                f"Scan completed - Processed {len(self.scanned_urls)} URLs, "
                f"Found {len(findings)} vulnerabilities"
            )

            return findings

        except Exception as e:
            self.logger.error(f"Error during web scan: {str(e)}")
            raise

    async def cleanup_scan(self) -> None:
        """Clean up after web scanning."""
        if self.session:
            await self.session.close()

    async def _crawl_site(self, base_url: str):
        """Crawl site for URLs to scan."""
        while self.found_urls:
            url = self.found_urls.pop()

            if len(self.scanned_urls) >= self.config["crawl_depth"] * 10:
                break

            try:
                async with self.rate_limiter:
                    async with self.session.get(url) as response:
                        self.stats["requests_made"] += 1

                        if response.status == 200:
                            content_type = response.headers.get("content-type", "")
                            if "text/html" in content_type.lower():
                                # Extract links
                                html = await response.text()
                                new_urls = self._extract_urls(html, base_url)

                                # Add new URLs to queue
                                for new_url in new_urls:
                                    if new_url not in self.scanned_urls:
                                        self.found_urls.add(new_url)
                                        self.stats["urls_found"] += 1

                yield url

            except Exception as e:
                self.logger.error(f"Error crawling {url}: {str(e)}")

    def _extract_urls(self, html: str, base_url: str) -> Set[str]:
        """Extract URLs from HTML content."""
        urls = set()
        soup = BeautifulSoup(html, "html.parser")

        # Find all links
        for tag in soup.find_all(["a", "link", "script", "img", "form"]):
            url = None
            if tag.name == "a" and tag.get("href"):
                url = tag["href"]
            elif tag.name in ["link", "script", "img"] and tag.get("src"):
                url = tag["src"]
            elif tag.name == "form" and tag.get("action"):
                url = tag["action"]

            if url:
                # Normalize URL
                full_url = urljoin(base_url, url)
                parsed = urlparse(full_url)

                # Only include URLs from same domain
                if (
                    parsed.netloc == urlparse(base_url).netloc
                    and parsed.path not in config.SCAN_TARGETS["web_interface"]["excluded_paths"]
                ):
                    urls.add(full_url)

        return urls

    async def _scan_url(self, url: str) -> List[VulnerabilityFinding]:
        """Scan URL for vulnerabilities."""
        findings = []

        try:
            # Test for XSS
            xss_findings = await self._check_xss(url)
            findings.extend(xss_findings)

            # Test for SQL injection
            sql_findings = await self._check_sql_injection(url)
            findings.extend(sql_findings)

            # Test for CSRF
            csrf_findings = await self._check_csrf(url)
            findings.extend(csrf_findings)

            # Test for SSRF
            ssrf_findings = await self._check_ssrf(url)
            findings.extend(ssrf_findings)

            # Test for file inclusion
            file_findings = await self._check_file_inclusion(url)
            findings.extend(file_findings)

            self.stats["vulnerabilities_found"] += len(findings)

        except Exception as e:
            self.logger.error(f"Error scanning {url}: {str(e)}")

        return findings

    async def _check_xss(self, url: str) -> List[VulnerabilityFinding]:
        """Check for XSS vulnerabilities."""
        findings = []
        payloads = [
            "<script>alert(1)</script>",
            "javascript:alert(1)",
            '"><img src=x onerror=alert(1)>',
            "';alert(1);//",
        ]

        for payload in payloads:
            try:
                # Test GET parameters
                params = {"q": payload, "search": payload, "id": payload}
                async with self.rate_limiter:
                    async with self.session.get(url, params=params) as response:
                        content = await response.text()
                        if payload in content:
                            findings.append(
                                VulnerabilityFinding(
                                    timestamp=datetime.now(),
                                    severity="high",
                                    title="Cross-Site Scripting (XSS)",
                                    description="XSS vulnerability found in GET parameter",
                                    vulnerability_type="xss",
                                    affected_component=url,
                                    evidence=f"Payload: {payload}",
                                    remediation="Implement proper input validation and output encoding",
                                    cwe_id="CWE-79",
                                )
                            )

                # Test POST parameters
                async with self.rate_limiter:
                    async with self.session.post(url, data={"input": payload}) as response:
                        content = await response.text()
                        if payload in content:
                            findings.append(
                                VulnerabilityFinding(
                                    timestamp=datetime.now(),
                                    severity="high",
                                    title="Cross-Site Scripting (XSS)",
                                    description="XSS vulnerability found in POST parameter",
                                    vulnerability_type="xss",
                                    affected_component=url,
                                    evidence=f"Payload: {payload}",
                                    remediation="Implement proper input validation and output encoding",
                                    cwe_id="CWE-79",
                                )
                            )

            except Exception as e:
                self.logger.error(f"Error testing XSS on {url}: {str(e)}")

        return findings

    async def _check_sql_injection(self, url: str) -> List[VulnerabilityFinding]:
        """Check for SQL injection vulnerabilities."""
        findings = []
        payloads = ["' OR '1'='1", "1' UNION SELECT NULL--", "1; DROP TABLE users--", "' OR 1=1--"]

        for payload in payloads:
            try:
                # Test GET parameters
                params = {"id": payload, "user_id": payload}
                async with self.rate_limiter:
                    async with self.session.get(url, params=params) as response:
                        content = await response.text()
                        if self._detect_sql_error(content):
                            findings.append(
                                VulnerabilityFinding(
                                    timestamp=datetime.now(),
                                    severity="critical",
                                    title="SQL Injection",
                                    description="SQL injection vulnerability found in GET parameter",
                                    vulnerability_type="sql_injection",
                                    affected_component=url,
                                    evidence=f"Payload: {payload}\nError: {self._extract_sql_error(content)}",
                                    remediation="Use parameterized queries and input validation",
                                    cwe_id="CWE-89",
                                )
                            )

                # Test POST parameters
                async with self.rate_limiter:
                    async with self.session.post(url, data={"input": payload}) as response:
                        content = await response.text()
                        if self._detect_sql_error(content):
                            findings.append(
                                VulnerabilityFinding(
                                    timestamp=datetime.now(),
                                    severity="critical",
                                    title="SQL Injection",
                                    description="SQL injection vulnerability found in POST parameter",
                                    vulnerability_type="sql_injection",
                                    affected_component=url,
                                    evidence=f"Payload: {payload}\nError: {self._extract_sql_error(content)}",
                                    remediation="Use parameterized queries and input validation",
                                    cwe_id="CWE-89",
                                )
                            )

            except Exception as e:
                self.logger.error(f"Error testing SQL injection on {url}: {str(e)}")

        return findings

    def _detect_sql_error(self, content: str) -> bool:
        """Detect SQL error messages in response."""
        error_patterns = [
            r"SQL syntax.*MySQL",
            r"Warning.*mysql_.*",
            r"PostgreSQL.*ERROR",
            r"ORA-[0-9][0-9][0-9][0-9]",
            r"Microsoft SQL Server",
            r"SQLite/JDBCDriver",
            r"SQLite\.Exception",
            r"System\.Data\.SQLite\.SQLiteException",
        ]

        return any(re.search(pattern, content, re.I) for pattern in error_patterns)

    def _extract_sql_error(self, content: str) -> str:
        """Extract SQL error message from response."""
        for line in content.split("\n"):
            if any(keyword in line.lower() for keyword in ["sql", "mysql", "postgresql", "sqlite"]):
                return line.strip()
        return "SQL error detected"

    async def _check_csrf(self, url: str) -> List[VulnerabilityFinding]:
        """Check for CSRF vulnerabilities."""
        findings = []

        try:
            async with self.rate_limiter:
                async with self.session.get(url) as response:
                    content = await response.text()

                    # Check for forms without CSRF token
                    soup = BeautifulSoup(content, "html.parser")
                    for form in soup.find_all("form", method=True):
                        if form["method"].lower() == "post":
                            has_csrf = False

                            # Check for common CSRF token field names
                            for input_field in form.find_all("input"):
                                field_name = input_field.get("name", "").lower()
                                if "csrf" in field_name or "token" in field_name:
                                    has_csrf = True
                                    break

                            if not has_csrf:
                                findings.append(
                                    VulnerabilityFinding(
                                        timestamp=datetime.now(),
                                        severity="high",
                                        title="Cross-Site Request Forgery (CSRF)",
                                        description="Form found without CSRF protection",
                                        vulnerability_type="csrf",
                                        affected_component=f"{url}#{form.get('id', '')}",
                                        evidence=str(form),
                                        remediation="Implement CSRF tokens for all forms",
                                        cwe_id="CWE-352",
                                    )
                                )

        except Exception as e:
            self.logger.error(f"Error checking CSRF on {url}: {str(e)}")

        return findings

    async def _check_ssrf(self, url: str) -> List[VulnerabilityFinding]:
        """Check for SSRF vulnerabilities."""
        findings = []
        test_urls = [
            "http://localhost",
            "http://127.0.0.1",
            "http://169.254.169.254",  # AWS metadata
            "http://[::1]",
            "file:///etc/passwd",
        ]

        for test_url in test_urls:
            try:
                params = {"url": test_url, "proxy": test_url, "uri": test_url}
                async with self.rate_limiter:
                    async with self.session.get(url, params=params) as response:
                        content = await response.text()

                        # Check for signs of successful SSRF
                        if any(
                            indicator in content
                            for indicator in [
                                "root:x:",  # /etc/passwd
                                "ami-id",  # AWS metadata
                                "localhost",
                                "127.0.0.1",
                            ]
                        ):
                            findings.append(
                                VulnerabilityFinding(
                                    timestamp=datetime.now(),
                                    severity="high",
                                    title="Server-Side Request Forgery (SSRF)",
                                    description=f"SSRF vulnerability found with URL: {test_url}",
                                    vulnerability_type="ssrf",
                                    affected_component=url,
                                    evidence=f"Test URL: {test_url}\nResponse contains sensitive data",
                                    remediation="Implement URL validation and whitelist allowed domains",
                                    cwe_id="CWE-918",
                                )
                            )

            except Exception as e:
                self.logger.error(f"Error checking SSRF on {url}: {str(e)}")

        return findings

    async def _check_file_inclusion(self, url: str) -> List[VulnerabilityFinding]:
        """Check for file inclusion vulnerabilities."""
        findings = []
        test_paths = [
            "../../../etc/passwd",
            "..\\..\\..\\windows\\win.ini",
            "/etc/passwd",
            "C:\\windows\\win.ini",
            "php://filter/convert.base64-encode/resource=index.php",
        ]

        for test_path in test_paths:
            try:
                params = {"file": test_path, "path": test_path, "include": test_path}
                async with self.rate_limiter:
                    async with self.session.get(url, params=params) as response:
                        content = await response.text()

                        # Check for signs of successful file inclusion
                        if any(
                            indicator in content
                            for indicator in ["root:x:", "[fonts]", "<?php", "#!/bin/bash"]
                        ):
                            findings.append(
                                VulnerabilityFinding(
                                    timestamp=datetime.now(),
                                    severity="high",
                                    title="File Inclusion Vulnerability",
                                    description=f"File inclusion vulnerability found with path: {test_path}",
                                    vulnerability_type="file_inclusion",
                                    affected_component=url,
                                    evidence=f"Test path: {test_path}\nResponse contains file contents",
                                    remediation="Implement proper input validation and restrict file access",
                                    cwe_id="CWE-98",
                                )
                            )

            except Exception as e:
                self.logger.error(f"Error checking file inclusion on {url}: {str(e)}")

        return findings
