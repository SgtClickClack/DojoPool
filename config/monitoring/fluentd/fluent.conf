# Input sources
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# Docker container logs
<source>
  @type tail
  path /var/lib/docker/containers/*/*.log
  pos_file /var/log/fluentd-docker.pos
  tag docker.*
  read_from_head true
  <parse>
    @type json
    time_key time
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

# Application logs
<source>
  @type tail
  path /var/log/dojopool/*.log
  pos_file /var/log/fluentd-app.pos
  tag dojopool.app
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

# Nginx access logs
<source>
  @type tail
  path /var/log/nginx/access.log
  pos_file /var/log/fluentd-nginx-access.pos
  tag nginx.access
  <parse>
    @type nginx
  </parse>
</source>

# Filter and enrich Docker logs
<filter docker.**>
  @type record_transformer
  <record>
    container_id ${tag_parts[1]}
    container_name ${record["container_name"]}
    environment ${record["environment"]}
  </record>
</filter>

# Output to Elasticsearch
<match **>
  @type elasticsearch
  host elasticsearch
  port 9200
  logstash_format true
  logstash_prefix fluentd
  include_tag_key true
  type_name fluentd
  tag_key @log_name
  flush_interval 1s
  <buffer>
    @type memory
    flush_interval 1s
    chunk_limit_size 2M
    queue_limit_length 32
    retry_max_interval 30
    retry_forever false
  </buffer>
</match>

# Backup to S3
<match **>
  @type copy
  <store>
    @type s3
    aws_key_id "#{ENV['AWS_ACCESS_KEY_ID']}"
    aws_sec_key "#{ENV['AWS_SECRET_ACCESS_KEY']}"
    s3_bucket dojopool-logs
    s3_region us-east-1
    path logs/
    time_slice_format %Y%m%d
    <buffer time>
      @type memory
      timekey 86400
      timekey_wait 10m
      chunk_limit_size 256m
    </buffer>
    <format>
      @type json
    </format>
  </store>
</match> 